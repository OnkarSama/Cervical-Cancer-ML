{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8970652,"sourceType":"datasetVersion","datasetId":5400611},{"sourceId":8970921,"sourceType":"datasetVersion","datasetId":5400786}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#invite people for the Kaggle party\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport random as rd\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-26T23:17:45.749558Z","iopub.execute_input":"2024-07-26T23:17:45.750249Z","iopub.status.idle":"2024-07-26T23:17:45.759455Z","shell.execute_reply.started":"2024-07-26T23:17:45.750205Z","shell.execute_reply":"2024-07-26T23:17:45.758008Z"},"trusted":true},"execution_count":303,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cervical-cancer-data/risk_factors_cervical_cancer.csv')\ndf_2 = pd.read_csv('/kaggle/input/behaviour-risk/sobar-72.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.764560Z","iopub.execute_input":"2024-07-26T23:17:45.765197Z","iopub.status.idle":"2024-07-26T23:17:45.790964Z","shell.execute_reply.started":"2024-07-26T23:17:45.765154Z","shell.execute_reply":"2024-07-26T23:17:45.789855Z"},"trusted":true},"execution_count":304,"outputs":[]},{"cell_type":"code","source":"df.head(900)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.793138Z","iopub.execute_input":"2024-07-26T23:17:45.793467Z","iopub.status.idle":"2024-07-26T23:17:45.823570Z","shell.execute_reply.started":"2024-07-26T23:17:45.793439Z","shell.execute_reply":"2024-07-26T23:17:45.822236Z"},"trusted":true},"execution_count":305,"outputs":[{"execution_count":305,"output_type":"execute_result","data":{"text/plain":"     Age Number of sexual partners First sexual intercourse  \\\n0     18                       4.0                     15.0   \n1     15                       1.0                     14.0   \n2     34                       1.0                        ?   \n3     52                       5.0                     16.0   \n4     46                       3.0                     21.0   \n..   ...                       ...                      ...   \n853   34                       3.0                     18.0   \n854   32                       2.0                     19.0   \n855   25                       2.0                     17.0   \n856   33                       2.0                     24.0   \n857   29                       2.0                     20.0   \n\n    Num of pregnancies Smokes Smokes (years) Smokes (packs/year)  \\\n0                  1.0    0.0            0.0                 0.0   \n1                  1.0    0.0            0.0                 0.0   \n2                  1.0    0.0            0.0                 0.0   \n3                  4.0    1.0           37.0                37.0   \n4                  4.0    0.0            0.0                 0.0   \n..                 ...    ...            ...                 ...   \n853                0.0    0.0            0.0                 0.0   \n854                1.0    0.0            0.0                 0.0   \n855                0.0    0.0            0.0                 0.0   \n856                2.0    0.0            0.0                 0.0   \n857                1.0    0.0            0.0                 0.0   \n\n    Hormonal Contraceptives Hormonal Contraceptives (years)  IUD  ...  \\\n0                       0.0                             0.0  0.0  ...   \n1                       0.0                             0.0  0.0  ...   \n2                       0.0                             0.0  0.0  ...   \n3                       1.0                             3.0  0.0  ...   \n4                       1.0                            15.0  0.0  ...   \n..                      ...                             ...  ...  ...   \n853                     0.0                             0.0  0.0  ...   \n854                     1.0                             8.0  0.0  ...   \n855                     1.0                            0.08  0.0  ...   \n856                     1.0                            0.08  0.0  ...   \n857                     1.0                             0.5  0.0  ...   \n\n    STDs: Time since first diagnosis STDs: Time since last diagnosis  \\\n0                                  ?                               ?   \n1                                  ?                               ?   \n2                                  ?                               ?   \n3                                  ?                               ?   \n4                                  ?                               ?   \n..                               ...                             ...   \n853                                ?                               ?   \n854                                ?                               ?   \n855                                ?                               ?   \n856                                ?                               ?   \n857                                ?                               ?   \n\n    Dx:Cancer Dx:CIN Dx:HPV Dx Hinselmann Schiller Citology Biopsy  \n0           0      0      0  0          0        0        0      0  \n1           0      0      0  0          0        0        0      0  \n2           0      0      0  0          0        0        0      0  \n3           1      0      1  0          0        0        0      0  \n4           0      0      0  0          0        0        0      0  \n..        ...    ...    ... ..        ...      ...      ...    ...  \n853         0      0      0  0          0        0        0      0  \n854         0      0      0  0          0        0        0      0  \n855         0      0      0  0          0        0        1      0  \n856         0      0      0  0          0        0        0      0  \n857         0      0      0  0          0        0        0      0  \n\n[858 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Number of sexual partners</th>\n      <th>First sexual intercourse</th>\n      <th>Num of pregnancies</th>\n      <th>Smokes</th>\n      <th>Smokes (years)</th>\n      <th>Smokes (packs/year)</th>\n      <th>Hormonal Contraceptives</th>\n      <th>Hormonal Contraceptives (years)</th>\n      <th>IUD</th>\n      <th>...</th>\n      <th>STDs: Time since first diagnosis</th>\n      <th>STDs: Time since last diagnosis</th>\n      <th>Dx:Cancer</th>\n      <th>Dx:CIN</th>\n      <th>Dx:HPV</th>\n      <th>Dx</th>\n      <th>Hinselmann</th>\n      <th>Schiller</th>\n      <th>Citology</th>\n      <th>Biopsy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18</td>\n      <td>4.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34</td>\n      <td>1.0</td>\n      <td>?</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>52</td>\n      <td>5.0</td>\n      <td>16.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>37.0</td>\n      <td>37.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>46</td>\n      <td>3.0</td>\n      <td>21.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>853</th>\n      <td>34</td>\n      <td>3.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>854</th>\n      <td>32</td>\n      <td>2.0</td>\n      <td>19.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>855</th>\n      <td>25</td>\n      <td>2.0</td>\n      <td>17.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.08</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>33</td>\n      <td>2.0</td>\n      <td>24.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.08</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>29</td>\n      <td>2.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>?</td>\n      <td>?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>858 rows Ã— 36 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['STDs: Time since first diagnosis'].describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.825022Z","iopub.execute_input":"2024-07-26T23:17:45.825387Z","iopub.status.idle":"2024-07-26T23:17:45.836889Z","shell.execute_reply.started":"2024-07-26T23:17:45.825356Z","shell.execute_reply":"2024-07-26T23:17:45.835834Z"},"trusted":true},"execution_count":306,"outputs":[{"execution_count":306,"output_type":"execute_result","data":{"text/plain":"count     858\nunique     19\ntop         ?\nfreq      787\nName: STDs: Time since first diagnosis, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df['STDs: Time since first diagnosis'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.839425Z","iopub.execute_input":"2024-07-26T23:17:45.839956Z","iopub.status.idle":"2024-07-26T23:17:45.851657Z","shell.execute_reply.started":"2024-07-26T23:17:45.839920Z","shell.execute_reply":"2024-07-26T23:17:45.850363Z"},"trusted":true},"execution_count":307,"outputs":[{"execution_count":307,"output_type":"execute_result","data":{"text/plain":"STDs: Time since first diagnosis\n?       787\n1.0      15\n3.0      10\n2.0       9\n4.0       6\n7.0       5\n16.0      4\n5.0       4\n8.0       3\n6.0       3\n19.0      2\n11.0      2\n21.0      2\n10.0      1\n22.0      1\n9.0       1\n12.0      1\n15.0      1\n18.0      1\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df = df.drop(columns=['STDs: Time since first diagnosis','STDs: Time since last diagnosis'])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.853026Z","iopub.execute_input":"2024-07-26T23:17:45.853384Z","iopub.status.idle":"2024-07-26T23:17:45.864712Z","shell.execute_reply.started":"2024-07-26T23:17:45.853353Z","shell.execute_reply":"2024-07-26T23:17:45.863435Z"},"trusted":true},"execution_count":308,"outputs":[]},{"cell_type":"code","source":"arr=[]\nfor i in df.itertuples():\n    tup = i\n    for j in tup:\n        if j == '?':\n            arr.append(tup[0])\n            break","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.866607Z","iopub.execute_input":"2024-07-26T23:17:45.867092Z","iopub.status.idle":"2024-07-26T23:17:45.887285Z","shell.execute_reply.started":"2024-07-26T23:17:45.867054Z","shell.execute_reply":"2024-07-26T23:17:45.885816Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"code","source":"index_df = pd.DataFrame(arr, columns=['indices'])\nindex_arr = index_df['indices'].unique().flatten()\n\ndf_cleaned = df.drop(index_arr)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.889752Z","iopub.execute_input":"2024-07-26T23:17:45.890149Z","iopub.status.idle":"2024-07-26T23:17:45.901427Z","shell.execute_reply.started":"2024-07-26T23:17:45.890116Z","shell.execute_reply":"2024-07-26T23:17:45.900116Z"},"trusted":true},"execution_count":310,"outputs":[]},{"cell_type":"code","source":"df_cleaned.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.903355Z","iopub.execute_input":"2024-07-26T23:17:45.903877Z","iopub.status.idle":"2024-07-26T23:17:45.946341Z","shell.execute_reply.started":"2024-07-26T23:17:45.903841Z","shell.execute_reply":"2024-07-26T23:17:45.945076Z"},"trusted":true},"execution_count":311,"outputs":[{"execution_count":311,"output_type":"execute_result","data":{"text/plain":"              Age  STDs: Number of diagnosis   Dx:Cancer      Dx:CIN  \\\ncount  668.000000                 668.000000  668.000000  668.000000   \nmean    27.264970                   0.092814    0.025449    0.004491   \nstd      8.727432                   0.310355    0.157603    0.066915   \nmin     13.000000                   0.000000    0.000000    0.000000   \n25%     21.000000                   0.000000    0.000000    0.000000   \n50%     26.000000                   0.000000    0.000000    0.000000   \n75%     33.000000                   0.000000    0.000000    0.000000   \nmax     84.000000                   3.000000    1.000000    1.000000   \n\n           Dx:HPV          Dx  Hinselmann    Schiller    Citology      Biopsy  \ncount  668.000000  668.000000  668.000000  668.000000  668.000000  668.000000  \nmean     0.023952    0.023952    0.044910    0.094311    0.058383    0.067365  \nstd      0.153015    0.153015    0.207262    0.292480    0.234642    0.250841  \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \nmax      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>STDs: Number of diagnosis</th>\n      <th>Dx:Cancer</th>\n      <th>Dx:CIN</th>\n      <th>Dx:HPV</th>\n      <th>Dx</th>\n      <th>Hinselmann</th>\n      <th>Schiller</th>\n      <th>Citology</th>\n      <th>Biopsy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>668.000000</td>\n      <td>668.000000</td>\n      <td>668.000000</td>\n      <td>668.000000</td>\n      <td>668.000000</td>\n      <td>668.000000</td>\n      <td>668.000000</td>\n      <td>668.000000</td>\n      <td>668.000000</td>\n      <td>668.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>27.264970</td>\n      <td>0.092814</td>\n      <td>0.025449</td>\n      <td>0.004491</td>\n      <td>0.023952</td>\n      <td>0.023952</td>\n      <td>0.044910</td>\n      <td>0.094311</td>\n      <td>0.058383</td>\n      <td>0.067365</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.727432</td>\n      <td>0.310355</td>\n      <td>0.157603</td>\n      <td>0.066915</td>\n      <td>0.153015</td>\n      <td>0.153015</td>\n      <td>0.207262</td>\n      <td>0.292480</td>\n      <td>0.234642</td>\n      <td>0.250841</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>13.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>21.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>26.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>33.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>84.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"arr=[]\nfor i in df_cleaned.itertuples():\n    tup = i\n    for j in tup:\n        if j == '?':\n            print(tup[0])\n            arr.append(tup[0])\n            break","metadata":{"execution":{"iopub.status.busy":"2024-07-26T23:17:45.948107Z","iopub.execute_input":"2024-07-26T23:17:45.948478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned['STDs:HPV'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned['Num of pregnancies'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned['Num of pregnancies'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Replace '?' with NaN\n# df['Num of pregnancies'].replace('?', 'np.nan', inplace=True)\n\n# # Step 2: Convert the column to numeric, forcing errors to NaN\n# df['Num of pregnancies'] = pd.to_numeric(df['Num of pregnancies'], errors='coerce')\n\n# df['Num of pregnancies'].describe()\n\n# # # Step 3: Calculate the mean, ignoring NaN values\n# mean_value = df['Num of pregnancies'].median()\n\n# print(mean_value)\n\ndf_cleaned['Hormonal Contraceptives'].unique()\n\n# Hormonal Contraceptives","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned['Hormonal Contraceptives (years)'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    feature_cols = ['Age', 'Number of sexual partners', 'First sexual intercourse',\n       'Num of pregnancies', 'Smokes', 'Smokes (years)', 'Smokes (packs/year)',\n       'Hormonal Contraceptives', 'Hormonal Contraceptives (years)', 'IUD',\n       'IUD (years)', 'STDs', 'STDs (number)', 'STDs:condylomatosis',\n       'STDs:cervical condylomatosis', 'STDs:vaginal condylomatosis',\n       'STDs:vulvo-perineal condylomatosis', 'STDs:syphilis',\n       'STDs:pelvic inflammatory disease', 'STDs:genital herpes',\n       'STDs:molluscum contagiosum', 'STDs:AIDS', 'STDs:HIV',\n       'STDs:Hepatitis B', 'STDs:HPV', 'STDs: Number of diagnosis',\n       'Dx:Cancer', 'Dx:CIN', 'Dx:HPV', 'Dx', 'Hinselmann', 'Schiller',\n       'Citology', 'Biopsy']\n# Step 2: Calculate the correlation matrix (it will automatically ignore NaN values)\ncorr_matrix = df_cleaned[feature_cols].astype(float).corr()\n\n# Step 3: Plot the heatmap\ncorr_matrix.style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned[df_cleaned['Dx']==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(df_total):\n    \n    full_cancerous = df_total[df_total['Dx'] == 1]\n    full_non_cancerous = df_total[df_total['Dx'] == 0]\n    \n    mid_cancerous = len(full_cancerous) // 2\n    mid_non_cancerous = len(full_non_cancerous) // 2\n    \n    cancerous1 = full_cancerous.iloc[:mid_cancerous]\n    cancerous2 = full_cancerous.iloc[mid_cancerous:]\n    non_cancerous1 = full_non_cancerous.iloc[:mid_non_cancerous]\n    non_cancerous2 = full_non_cancerous.iloc[mid_non_cancerous:]\n    \n    return cancerous1.reset_index(drop=True), cancerous2.reset_index(drop=True), non_cancerous1.reset_index(drop=True), non_cancerous2.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_random_indices(num_of_random,len_of_df):\n    \n    indices = []\n    \n    for i in range(num_of_random):\n        indices.append(rd.randint(0,len_of_df-1))\n    \n    return indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_test_set(cancerous1, cancerous2, non_cancerous1, non_cancerous2):\n    \n    random_nc = pd.DataFrame()\n    X_train = pd.DataFrame()\n    X_test = pd.DataFrame()\n    Y_train = pd.DataFrame()\n    Y_test = pd.DataFrame()\n    \n    random_indices = get_random_indices(8,len(non_cancerous1))\n    \n    random_nc = non_cancerous1.iloc[random_indices]\n    \n    full_train_set = pd.concat([cancerous1,random_nc])\n    \n    full_test_set = pd.concat([cancerous2,non_cancerous2])\n    \n    feature_cols = ['Age', 'Number of sexual partners', 'First sexual intercourse',\n       'Num of pregnancies', 'Smokes', 'Smokes (years)', 'Smokes (packs/year)',\n       'Hormonal Contraceptives', 'Hormonal Contraceptives (years)', 'IUD',\n       'IUD (years)', 'STDs', 'STDs (number)', 'STDs:condylomatosis',\n       'STDs:cervical condylomatosis', 'STDs:vaginal condylomatosis',\n       'STDs:vulvo-perineal condylomatosis', 'STDs:syphilis',\n       'STDs:pelvic inflammatory disease', 'STDs:genital herpes',\n       'STDs:molluscum contagiosum', 'STDs:AIDS', 'STDs:HIV',\n       'STDs:Hepatitis B', 'STDs:HPV', 'STDs: Number of diagnosis',\n       'Dx:Cancer', 'Dx:CIN', 'Dx:HPV', 'Hinselmann', 'Schiller',\n       'Citology', 'Biopsy']\n    \n    X_train = full_train_set[feature_cols]\n    Y_train = pd.concat([Y_train,full_train_set['Dx']])\n    \n    X_test = full_test_set[feature_cols]\n    Y_test = pd.concat([Y_test,full_test_set['Dx']])\n    \n    return X_train.reset_index(drop=True), Y_train.reset_index(drop=True), X_test.reset_index(drop=True), Y_test.reset_index(drop=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_coefs(X_train,Y_train):\n    \n    num_of_cols = len(X_train.columns)\n    \n    columns_names = [f'x{i+1}' for i in range(num_of_cols)] + ['intercept']\n    \n    coef_list = pd.DataFrame()\n    \n    logreg = LogisticRegression()\n\n    # fit the model with data\n    logreg.fit(X_train, Y_train)\n\n    coef_intercept = np.append(logreg.coef_.flatten(),logreg.intercept_[0])\n    \n    coef_intercept = coef_intercept.reshape(1, -1)\n    \n    coef_list = pd.DataFrame(coef_intercept, columns=columns_names)\n    \n    return coef_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_simulation(sim_len, df_total):\n    \n    sim_coef_list = pd.DataFrame()\n    \n    cancerous1, cancerous2, non_cancerous1, non_cancerous2 = split_data(df_total)\n    \n    for i in range(sim_len):\n        \n        X_train, Y_train, X_test, Y_test = create_train_test_set(cancerous1, cancerous2, non_cancerous1, non_cancerous2) \n        \n        coef_list = get_coefs(X_train,Y_train)\n        \n        sim_coef_list = pd.concat([coef_list,sim_coef_list])\n        \n    return sim_coef_list.reset_index(drop=True), X_test, Y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sim_coef_list, X_test, Y_test = run_simulation(100, df_cleaned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sim_coef_list.head(500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_avg_coefs_with_df(sim_coef_list):\n    # Compute the mean of each column in the DataFrame\n    avg_coefs = sim_coef_list.mean()\n    \n    # Convert the resulting Series to a NumPy array\n    avg_coefs_array = avg_coefs.to_numpy()\n    \n    # Create a DataFrame from the average coefficients\n    avg_coefs_df = pd.DataFrame([avg_coefs], columns=sim_coef_list.columns, index=['avg'])\n    \n    # Concatenate the original DataFrame with the average coefficients DataFrame\n    result_df = pd.concat([sim_coef_list, avg_coefs_df], ignore_index=False)\n    \n    return result_df, avg_coefs_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_df, avg_arr= get_avg_coefs_with_df(sim_coef_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_df.head(200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_arr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_df.head(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\ndef evaluate_logistic_regression(X_test, y_test, coefs):\n    \"\"\"\n    Evaluate logistic regression performance on test data using provided coefficients,\n    and plot relevant metrics including the sigmoid function, confusion matrix, ROC curve.\n    \n    Parameters:\n    X_test (pd.DataFrame or np.ndarray): Test feature data.\n    y_test (pd.Series or np.ndarray): True labels for the test data.\n    coefs (np.ndarray): Coefficients including the intercept as the last element.\n    \n    Returns:\n    dict: Performance metrics including accuracy, precision, recall, F1 score, and ROC AUC.\n    \"\"\"\n    \n    X_test = X_test.apply(pd.to_numeric, errors='coerce')\n    \n    # Extract feature coefficients and intercept\n    intercept = coefs[-1]\n    feature_coefs = coefs[:-1]\n    \n    # Compute logits (linear combination of features and coefficients)\n    logits = np.dot(X_test, feature_coefs) + intercept\n    \n    y_vals = []\n    \n    for i in logits:\n        if i > 0:\n            y_vals.append(1)\n        else:\n            y_vals.append(0)\n    \n    # Compute predicted probabilities using the logistic function\n    probabilities = 1 / (1 + np.exp(-logits))\n    \n    # Convert probabilities to binary predictions using a threshold (e.g., 0.5)\n    predictions = (probabilities > 0.5).astype(int)\n    \n    # Calculate performance metrics\n    metrics = {\n        'Accuracy': accuracy_score(y_test, predictions),\n        'Precision': precision_score(y_test, predictions),\n        'Recall': recall_score(y_test, predictions),\n        'F1 Score': f1_score(y_test, predictions),\n        'ROC AUC': roc_auc_score(y_test, probabilities)\n    }\n    \n    # Plot the sigmoid function and overlay computed z values\n    z = logits  # z values based on the linear combination\n    sigmoid = 1 / (1 + np.exp(-z))\n    \n    # Plot Sigmoid Function\n    plt.figure(figsize=(8, 4))\n    z_values = np.linspace(-7.5, 5, 400)\n    sigmoid_curve = 1 / (1 + np.exp(-z_values))\n    plt.plot(z_values, sigmoid_curve, label='Sigmoid Function Curve')\n    plt.scatter(z, y_vals, color='red', s=50, alpha=0.7, label='Computed Sigmoid Values')\n    plt.title('Sigmoid Function with Computed Values')\n    plt.xlabel('Logits (z)')\n    plt.ylabel('Probability')\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n    \n    # Plot confusion matrix\n    cm = confusion_matrix(y_test, predictions)\n    cm_df = pd.DataFrame(cm, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive'])\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.show()\n    \n    # Plot ROC curve\n    fpr, tpr, _ = roc_curve(y_test, probabilities)\n    auc = roc_auc_score(y_test, probabilities)\n    plt.figure(figsize=(8, 4))\n    plt.plot(fpr, tpr, color='blue', lw=2, label=\"auc=\"+str(auc))\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n    plt.title('ROC Curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    return metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = evaluate_logistic_regression(X_test, Y_test, avg_arr)\nprint(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now let's create a function that returns the AIC. \ndef calculate_AIC(n,mse,num_params):\n    aic= n * math.log(mse) + 2 * num_params\n    return aic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = len(X_test)\nmse = 1 - metrics['Accuracy']\nnum_params = len(X_test.columns)\naic_score = calculate_AIC(n,mse,num_params)\nprint(aic_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mod_create_train_test_set(cancerous1, cancerous2, non_cancerous1, non_cancerous2, feature_cols):\n    \n    random_nc = pd.DataFrame()\n    X_train = pd.DataFrame()\n    X_test = pd.DataFrame()\n    Y_train = pd.DataFrame()\n    Y_test = pd.DataFrame()\n    \n    random_indices = get_random_indices(8,len(non_cancerous1))\n    \n    random_nc = non_cancerous1.iloc[random_indices]\n    \n    full_train_set = pd.concat([cancerous1,random_nc])\n    \n    full_test_set = pd.concat([cancerous2,non_cancerous2])\n    \n    X_train = full_train_set[feature_cols]\n    Y_train = pd.concat([Y_train,full_train_set['Dx']])\n    \n    X_test = full_test_set[feature_cols]\n    Y_test = pd.concat([Y_test,full_test_set['Dx']])\n    \n    return X_train.reset_index(drop=True), Y_train.reset_index(drop=True), X_test.reset_index(drop=True), Y_test.reset_index(drop=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mod_run_simulation(sim_len, df_total, feature_cols):\n    \n    sim_coef_list = pd.DataFrame()\n    \n    cancerous1, cancerous2, non_cancerous1, non_cancerous2 = split_data(df_total)\n    \n    for i in range(sim_len):\n        \n        X_train, Y_train, X_test, Y_test = mod_create_train_test_set(cancerous1, cancerous2, non_cancerous1, non_cancerous2, feature_cols) \n        \n        coef_list = get_coefs(X_train,Y_train)\n        \n        sim_coef_list = pd.concat([coef_list,sim_coef_list])\n        \n    return sim_coef_list.reset_index(drop=True), X_test, Y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\ndef mod_evaluate_logistic_regression(X_test, y_test, coefs):\n    \n    \"\"\"\n    Evaluate logistic regression performance on test data using provided coefficients,\n    and plot relevant metrics including the sigmoid function, confusion matrix, ROC curve.\n    \n    Parameters:\n    X_test (pd.DataFrame or np.ndarray): Test feature data.\n    y_test (pd.Series or np.ndarray): True labels for the test data.\n    coefs (np.ndarray): Coefficients including the intercept as the last element.\n    \n    Returns:\n    dict: Performance metrics including accuracy, precision, recall, F1 score, and ROC AUC.\n    \"\"\"\n    \n    X_test = X_test.apply(pd.to_numeric, errors='coerce')\n    \n    # Extract feature coefficients and intercept\n    intercept = coefs[-1]\n    feature_coefs = coefs[:-1]\n    \n    # Compute logits (linear combination of features and coefficients)\n    logits = np.dot(X_test, feature_coefs) + intercept\n    \n    # Compute predicted probabilities using the logistic function\n    probabilities = 1 / (1 + np.exp(-logits))\n    \n    # Convert probabilities to binary predictions using a threshold (e.g., 0.5)\n    predictions = (probabilities > 0.5).astype(int)\n    \n    acc = accuracy_score(y_test, predictions)\n    \n    n = len(X_test)\n    mse = 1 - acc\n    num_params = len(X_test.columns)\n    \n    # Calculate performance metrics\n    metrics = {\n        'Accuracy': acc,\n        'Precision': precision_score(y_test, predictions),\n        'Recall': recall_score(y_test, predictions),\n        'F1 Score': f1_score(y_test, predictions),\n        'ROC AUC': roc_auc_score(y_test, probabilities),\n        'AIC Score': calculate_AIC(n,mse,num_params)\n    }\n    \n    return metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\ndef finding_optimal_attributes(df_total, sim_len, max_cols):\n    \n    feature_cols = ['Age', 'Number of sexual partners', 'First sexual intercourse',\n       'Num of pregnancies', 'Smokes', 'Smokes (years)', 'Smokes (packs/year)',\n       'Hormonal Contraceptives', 'Hormonal Contraceptives (years)', 'IUD',\n       'IUD (years)', 'STDs', 'STDs (number)', 'STDs:condylomatosis',\n       'STDs:cervical condylomatosis', 'STDs:vaginal condylomatosis',\n       'STDs:vulvo-perineal condylomatosis', 'STDs:syphilis',\n       'STDs:pelvic inflammatory disease', 'STDs:genital herpes',\n       'STDs:molluscum contagiosum', 'STDs:AIDS', 'STDs:HIV',\n       'STDs:Hepatitis B', 'STDs:HPV', 'STDs: Number of diagnosis',\n       'Dx:Cancer', 'Dx:CIN', 'Dx:HPV', 'Hinselmann', 'Schiller',\n       'Citology', 'Biopsy']\n\n    best_combination = None\n    best_accuracy = 0\n    best_aic = float('inf')\n\n    results = []\n\n    for L in range(1, max_cols):\n        for subset in itertools.combinations(feature_cols, L):\n            \n            df_optimized_x = df_total[list(subset)].reset_index(drop=True)\n            df_optimized_y = df_total[['Dx']].reset_index(drop=True)\n            df_optimized = pd.concat([df_optimized_x, df_optimized_y], axis=1)\n\n            sim_coef_list, X_test, Y_test = mod_run_simulation(sim_len, df_optimized, list(subset))\n\n            avg_df, avg_arr = get_avg_coefs_with_df(sim_coef_list)\n\n            metrics = mod_evaluate_logistic_regression(X_test, Y_test, avg_arr)\n\n            results.append({\n                'features': subset,\n                'accuracy': metrics['Accuracy'],\n                'aic': metrics['AIC Score']\n            })\n\n            if metrics['Accuracy'] >= best_accuracy and metrics['AIC Score'] <= best_aic:\n                best_accuracy = metrics['Accuracy']\n                best_aic = metrics['AIC Score']\n                best_combination = subset\n\n    results_df = pd.DataFrame(results)\n    return results_df, best_combination, best_accuracy, best_aic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = df_cleaned.astype(float).corr()\ncorr_matrix.head(35)\ncorr_matrix.style.background_gradient(cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) >=0.8)]\n\nto_drop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned_2 = df_cleaned.drop(to_drop, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sim_len = 100\ndf_total = df_cleaned_2\nfeature_cols = df_total.columns\nsim_coef_list_2, X_test_2, Y_test_2 = mod_run_simulation(sim_len, df_total, feature_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_df_2, avg_arr_2 = get_avg_coefs_with_df(sim_coef_list_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_2 = evaluate_logistic_regression(X_test_2, Y_test_2, avg_arr_2)\nprint(metrics_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \nn = len(X_test_2)\nmse = 1 - metrics_2['Accuracy']\nnum_params = len(X_test_2.columns)\n\ndef calculate_AIC(n,mse,num_params):\n    aic= n * math.log(mse) + 2 * num_params\n    return aic\n\naic = calculate_AIC(n,mse,num_params)\nprint(aic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cleaned_3 = df_cleaned_2.drop('STDs:Hepatitis B',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sim_len = 100\ndf_total = df_cleaned_3\nfeature_cols = df_total.columns\nsim_coef_list_3, X_test_3, Y_test_3 = mod_run_simulation(sim_len, df_total, feature_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_df_3, avg_arr_3 = get_avg_coefs_with_df(sim_coef_list_3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_3 = evaluate_logistic_regression(X_test_3, Y_test_3, avg_arr_3)\nprint(metrics_3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = len(X_test_3)\nmse = 1 - metrics_3['Accuracy']\nnum_params = len(X_test_3.columns)\n\ndef calculate_AIC(n,mse,num_params):\n    aic= n * math.log(mse) + 2 * num_params\n    return aic\n\naic = calculate_AIC(n,mse,num_params)\nprint(aic)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_standard_deviation(df_name):\n    \n    df_std = df_name.std(axis=0)\n    \n    df_name = df_name.concat([df_name,df_std],keys=['std'])\n    \n    return df_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_std = get_standard_deviation(avg_df_3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_std.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n#df_optimized_x,df_optimized_y, df_optimized = finding_optimal_attributes(df_cleaned, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n#df_optimized_x.head(668)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n#df_optimized_y.head(668)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n#df_optimized.head(690)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test\n#df_optimized[df_optimized['Dx']==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Example usage:\n# results_df, best_combination, best_accuracy, best_aic = finding_optimal_attributes(df_cleaned, 100)\n# print(results_df)\n# print(f\"Best combination: {best_combination}, Accuracy: {best_accuracy}, AIC: {best_aic}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sim_len = 100\n# max_cols = 20\n\n# results_df, best_combination, best_accuracy, best_aic = finding_optimal_attributes(df_cleaned, 1, 5)\n# print(results_df)\n# print(f\"Best combination: {best_combination}, Accuracy: {best_accuracy}, AIC: {best_aic}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}